\subsection{Explanation of SCAMs}

SCAMs \parencite{Pya2010,Pya2015} are an extenstion of GAMs.  Instead of thin plate regression splines, they use penalized splines (P-splines) \parencite{Eilers1996} to construct their smooth function basis, which in turn build upon basis splines (B-splines) \parencite{Boor2001,Curry1947}.

\subsubsection{B-splines and P-splines}
\label{sec:BSplinesPSplines}

A B-spline is a pieceweise polyonmial.  Adjacent polynomial pieces are joined at the so-called knots in a specific manner:  both the ordinates as well as the first derivatives of the adjacent polynomials are equal at the knots \parencite{Eilers1996}.  This ensures that the resulting B-spline is continuous in its value and first derivative.  To define a B-spline, let the knot vector \(\symbf{T} = \bigl(t_0, t_1, \ldots, t_m\bigr)\) be a nondecreasing sequence with \(t_i \in [0, 1]\) and define the control points \(\symbf{P}_0, \ldots, \symbf{P}_n\).  The degree is defined as \(p \equiv m - n - 1\).  Each control point is associated with a basis function, which is given by
\begin{equation}
  \label{eq:BSplineBasisFunctionsDegreeZero}
  N_{i, 0}(x) =
  \begin{cases}
    1 &\text{if } t_i \leq x < t_{i + 1} \text{ and } t_i < t_{i + 1} \\
    0 &\text{otherwise}
  \end{cases},
\end{equation}
and
\begin{equation}
  \label{eq:BSplineBasisFunctionsDegreeNonzero}
  N_{i, j}(x) = \frac{x - t_i}{t_{i + j} - t_i} N_{i, j - 1}(x) + \frac{t_{i + j + 1} - x}{t_{i + j + 1} - t_{i + 1}} N_{i + 1, j - 1}(x) \quad \text{for } j = 1, 2, \ldots, p.
\end{equation}
Then the curve defined by
\begin{equation}
  \label{eq:BSplineCurveDefinition}
  \symbf{C}(x) = \sum_{i = 0}^n \symbf{P}_i N_{i, p}(x)
\end{equation}
is a B-spline \parencite{Weisstein2017a}.  This can then be used for regression by estimating the vector of control points.  Commonly, this is done via the method of ordinary least squares \parencite{Bollaerts2006} by estimating the control points which, for \(q\) observations \((x_j, y_j)\), minimize the equation
\begin{equation}
  \label{eq:BSplineOLS}
  S =
  \sum_{j = 1}^q 
  \Biggl(
  y_j - \sum_{i = 0}^n \symbf{P}_i N_{i, p}\bigl(x_j\bigr)
  \Biggr)^2.
\end{equation}
The concept of P-splines extends B-splines in 2 ways: on the one hand it sets a large number of equidistant knots, while on the other hand adding a penalty to \Cref{eq:BSplineOLS} in order to prevent overfitting.  This penalty is based on high-order (\(k \geq 2\)) finite differences between the coefficients of adjacent B-splines.  The least squares objective function to minimize thus becomes
\begin{equation}
  \label{eq:PSplineOLS}
  S =
  \sum_{j = 1}^q 
  \Biggl(
  y_j - \sum_{i = 0}^n \symbf{P}_i N_{i, p}\bigl(x_j\bigr)
  \Biggr)^2
  + \lambda \sum_{i = k + 1}^n \bigl(\upDelta^k \symbf{P}_i\bigr)^2,
\end{equation}
with \(\upDelta^k \symbf{P}_i\) being the \(k\)th-order differences, i.e., \(\upDelta^k\symbf{P}_i = \upDelta^1 \bigl(\upDelta^{k - 1} \symbf{P}_i\bigr)\) with \(\upDelta^1 \symbf{P}_i = \symbf{P}_i - \symbf{P}_{i - 1}\) and with \(\lambda\) being a smoothness parameter \parencite{Eilers1996, Bollaerts2006}.

\subsubsection{SCAMs}

A SCAM may have a structure like
\begin{equation}
  \label{eq:SCAM}
  g\bigl(\mu_i\bigr) = \symbf{A} \symbf{\theta} + \sum_j f_j\bigl(z_{j, i}\bigr) + \sum_k m_k\bigl(x_{k, i}\bigr),
\end{equation}
where \(g\) is a known smooth monotonic link function, \(\mu_i\) is the mean of the univariate response variable which follows an exponential family distribution, \(\symbf{A}\) is the model matrix, \(\symbf{\theta}\) is a vector of unknown parameters, \(f_j\) is an unknown smooth function of predictor variable \(z_j\) and \(m_k\) is an unknown shape constrained smooth function of predictor variable \(x_k\) \parencite{Pya2015}.

%% CONTINUE HERE with an explanation of B-splines (cp. de Boor (2001)) and of P-splines (cp. Eilers & Marx (1996))

In order to explain shape constrained smooth functions, consider the case of a monotonically increasing smooth, \(m\), using a B-spline basis.  Let
\begin{equation}
  \label{eq:SCAMMonotonicallyIncreasingSmooth}
  m(x) = \sum_{j = 1}^q \gamma_j B_j(x),
\end{equation}
where \(q\) is the number of basis functions, the \(B_j\) are B-spline basis functions of at least second order for representing smooth functions over interval \(\interval{a}{b}\), based on equally spaced knots, and the \(\gamma_j\) are spline coefficients \parencite{Pya2015}.  A sufficient condition for ensuring the smooth function \(m\) to be monotonically increasing (i.e., for \(m'(x) \geq 0\)) over \(\interval{a}{b}\) is that \(\gamma_j \geq \gamma_{j - 1} \forall j\).  This condition can be imposed by reparameterizing, so that
\begin{equation}
  \label{eq:SCAMReparameterizedGamma}
  \symbf{\gamma} = \symbf{\Sigma} \tilde{\symbf{\beta}},
\end{equation}
where \(\symbf{\beta} = \bigl[\beta_1, \beta_2, \ldots, \beta_q\bigr]^{\text{T}}\) and \(\tilde{\symbf{\beta}} = \bigl[\beta_1, \exp(\beta_2), \ldots, \exp(\beta_q)\bigr]^{\text{T}}\), while
% \(\Sigma_{i, j} = 0\) if \(i < j\) and \(\Sigma_{i, j} = 1\) if \(i \geq j\).
\begin{equation}
  \label{eq:SCAMSigmaMontoneIncreasing}
  \Sigma_{i, j} =
  \begin{cases}
    0 &\text{if } i < j \\
    1 &\text{if } i \geq j \\
  \end{cases}
\end{equation}
\parencite{Pya2015}.
Thus, if \(\symbf{m} = [m(x_1), m(x_2), \ldots, m(x_n)]^{\text{T}}\) is the vector of \(m\) values at the observed points \(x_i\) and \(\symbf{X}\) is a matrix such that \(X_{i, j} = B_j(x_i)\), then
\begin{equation}
  \label{eq:SCAMConstrainedSmootherVector}
  \symbf{m} = \symbf{X} \symbf{\Sigma} \tilde{\symbf{\beta}}
\end{equation}
\parencite{Pya2015}.
Smoothness of \(m\) is ensured by penalizing the squared difference between adjacent \(\beta_j\), starting from \(\beta_2\), using \(\norm{\symbf{D} \symbf{\beta}}^2\) as the penalty, where \(\symbf{D}\) is the \((q-2) \times q\) matrix
% which is all zero except that \(D_{i, i + 1} = - D_{i, i + 2} = 1\) for \(i = 1, \ldots, q - 2\).
whose elements are given by
\begin{equation}
  \label{eq:SCAMDMonotoneIncreasing}
  D_{i, j} = 
  \begin{cases}
    - D_{i, j + 1} = 1 &\text{if } i = 1, \ldots, q - 2,~ j = i + 1 \\
    0 &\text{otherwise}
  \end{cases}.
\end{equation}
This penalty becomes zero when all \(\beta_j\) following \(\beta_1\) are equal, thus ensuring the \(\gamma_j\) to form a uniformly increasing sequence and \(m(x)\) to form an increasing straight line \parencite{Pya2015}.

Embedding the shape constrained smooth function \(m(x)\) in a larger model requires an additional constraint on \(m(x)\) in order to avoid it being confused with the intercept of the larger model.  This can be achieved by imposing centering constraints on the model matrix columns, i.e., by setting the sum of the values of the smooth to zero: \(\sum_{i = 1}^n m(x_i) = 0\) \parencite{Pya2015}.

The SCAMs presented in this study constrain the smooth function to be increasing and concave.  For this constraint, matrix \(\symbf{\Sigma}\) has to be adjusted so that
\begin{equation}
  \label{eq:SCAMSigmaIncreasingConcave}
  \Sigma_{i, j} =
  \begin{cases}
    0 &\text{if } i = 1,~ j \geq 2 \\
    1 &\text{if } i \geq 1,~ j = 1 \\
    i - 1 &\text{if } i \geq 2,~ j = 2, \ldots, q - 1 + 2 \\
    q - j + 1 &\text{if } i \geq 2,~ j = q - i + 3, \ldots, q,\\
  \end{cases}
\end{equation}
while matrix \(\symbf{D}\) has to be adjusted so that
\begin{equation}
  \label{eq:SCAMDIncreasingConcave}
  D_{i, j} = 
  \begin{cases}
    - D_{i, j + 1} = 1 &\text{if } i = 1, \ldots, q - 3,~ j = i + 2 \\
    0 &\text{otherwise}
  \end{cases}
\end{equation}
\parencite{Pya2015}.

In order to represent \Cref{eq:SCAM} for computation, the following paragraphs use basis expansion, penalties, and identifiability constraints for all \(f_j\) as described in \textcite{Wood2006}.  Thus, 
\begin{equation}
  \label{eq:SCAMCombinedModelMatricesfk}
  \sum_j f_j\bigl(z_{j, i}\bigr) = \symbf{F}_i \symbf{\gamma},
\end{equation}
where \(\symbf{F}\) is a model matrix determined by the basis functions and the constraints and \(\symbf{\gamma}\) is a vector of coefficients to be estimated.  The penalties on the \(f_j\) are quadratic in \(\symbf{\gamma}\).  Each \(m_k\) is represented by a model matrix of the form \(\symbf{X} \symbf{\Sigma}\) and a corresponding coefficient vector \parencite{Pya2015}.  The model matrices for all \(m_k\) are then combined so that
\begin{equation}
  \label{eq:SCAMCombinedModelMatricesMk}
  \sum_k m_k\bigl(x_{k, i}\bigr) = \symbf{M}_i \tilde{\symbf{\beta}},
\end{equation}
where \(\symbf{M}\) is a model matrix and \(\tilde{\symbf{\beta}}\) is a vector containing both model coefficients (\(\beta_i\)) and exponentiated model coefficients (\(\exp(\beta_i)\)).  The penalties are quadratic in the coefficients \(\symbf{\beta}\) (not in the \(\tilde{\symbf{\beta}}\)).  Thus, \Cref{eq:SCAM} becomes
\begin{equation}
  \label{eq:SCAMComputationalRepresentation01}
  g\bigl(\mu_i\bigr) = \symbf{A}_i \symbf{\theta} + \symbf{F}_i \symbf{\gamma} + \symbf{M}_i \tilde{\symbf{\beta}}.
\end{equation}
For fitting purposes, the model matrices may be combined column-wise into a single model matrix \(\symbf{X}\). Thus, \Cref{eq:SCAMComputationalRepresentation01} becomes
\begin{equation}
  \label{eq:SCAMComputationalRepresentation02}
  g\bigl(\mu_i\bigr) = \symbf{X}_i \tilde{\symbf{\beta}},
\end{equation}
where \(\tilde{\symbf{\beta}}\) has been enlarged to now contain \(\symbf{\theta}\), \(\symbf{\gamma}\), and the original \(\tilde{\symbf{\beta}}\).  Similarly there is a corresponding expanded model coefficient vector \(\symbf{\beta}\) containing \(\symbf{\theta}\), \(\symbf{\gamma}\), and the original \(\symbf{\beta}\).  The penalties on the terms have the general form \(\symbf{\beta}^{\text{T}} \symbf{S}_\lambda \symbf{\beta}\), where \(\symbf{S}_\lambda = \sum_k \uplambda_k \symbf{S}_k\), and the \(\symbf{S}_k\) are the original penalty matrices expanded with zeros everywhere except for the elements which correspond to the coefficients of the \(k\)th smooth \parencite{Pya2015}.

The chosen probability distribution determines the form of the log-likelihood \(\loglikelihood(\symbf{\beta})\) of the model.  In order to control model smoothness, the penalized version of the log-likelihood
\begin{equation}
  \label{eq:SCAMPenalizedLogLikelihood}
  \loglikelihood_p(\symbf{\beta}) = \loglikelihood(\symbf{\beta}) - \frac{\symbf{\beta}^{\text{T}} \symbf{S}_\uplambda \symbf{\beta}}{2}
\end{equation}
needs to be maximized.  For this, let \(V(\mu)\) be the variance of the chosen probability distribution, and define
\begin{equation}
  \label{eq:SCAMVarianceAlpha}
  \alpha \bigl(\mu_i\bigr) = 1 + \bigl(y_i - \mu_i\bigr)\Biggl\{\frac{V'\bigl(\mu_i\bigr)}{V\bigl(\mu_i\bigr)} + \frac{g''\bigl(\mu_i\bigr)}{g'\bigl(\mu_i\bigr)}\Biggr\}.
\end{equation}
Maximization of the penalized log-likelihood is then achieved in the following way \parencite{Pya2015}:
\begin{enumerate}
\item Obtain an initial estimate of \(\symbf{\beta}\) by minimizing \(\norm{g(\symbf{y}) - \symbf{X} \tilde{\symbf{\beta}}}^2 + {\tilde{\symbf{\beta}}}^{\text{T}} S_\uplambda \tilde{\symbf{\beta}}\) with respect to \(\tilde{\symbf{\beta}}\), subject to linear inequality constraints which ensure that \(\tilde{\beta}_j > 0\) whenever \(\tilde{\beta}_j = \exp(\tilde{\beta})\).
\item Set \(k = 0\) and repeat steps 3--11 until convergence.
\item Evaluate \(z_i = \left. \bigl(y_i - \mu_i\bigr) g'\bigl(\mu_i\bigr) \middle/ \alpha\bigl(\mu_i\bigr) \right.\) and \(w_i = \left. \omega_i \alpha\bigl(\mu_i\bigr) \middle/ \bigl\{V\bigl(\mu_i\bigr) g'^2\bigl(\mu_i\bigr)\bigr\} \right.\), using the current estimate of \(\mu_i\).
\item Evaluate vectors \(\tilde{\symbf{w}} = |\symbf{w}|\) and \(\tilde{\symbf{z}}\), where \(\tilde{z}_i = \sign\left(w_i\right) z_i\).
\item Evaluate diagonal matrix \(\symbf{C}\) such that
  % \(C_{j, j} = 1\) if \(\tilde{\beta}_j = \beta_j\), and \(C_{j, j} = \exp\bigl(\beta_j\bigr)\) otherwise.
  \(C_{j, j} =
  \begin{cases}
    1 &\text{if } \tilde{\beta}_j = \beta_j \\
    \exp\bigl(\beta_j\bigr) & \text{otherwise}.
  \end{cases}
  \)
\item Evaluate diagonal matrix \(\symbf{E}\) such that
  % \(E_{j, j} = 0\) if \(\tilde{\beta}_j = \beta_j\), and \\
  % \(E_{j, j} = \left. \sum_i^n w_i g'\bigl(\mu_i\bigr) [\symbf{X} \symbf{C}]_{i, j} \bigl(y_i - \mu_i\bigr) \middle/ \alpha\bigl(\mu_i\bigr) \right.\) otherwise.
  \\ \(E_{j, j} =
  \begin{cases}
    0 &\text{if } \tilde{\beta}_j = \beta_j \\
    \left. \sum_i^n w_i g'\bigl(\mu_i\bigr) [\symbf{X} \symbf{C}]_{i, j} \bigl(y_i - \mu_i\bigr) \middle/ \alpha\bigl(\mu_i\bigr) \right. &\text{otherwise}.
  \end{cases}
\)
\item Let \(\symbf{I}^-\) be a diagonal matrix such that
  % \(I_{i, i}^- = 1\) if \(w_i < 0\) and \(I_{i, i}^- = 0\) otherwise.
  \(I_{i, i}^- = 
  \begin{cases}
    1 &\text{if } w_i < 0 \\
    0 &\text{otherwise}.
  \end{cases}
\)
\item Letting \(\tilde{\symbf{W}}\) denote \(\diag\bigl(\tilde{\symbf{w}}\bigr)\), form the QR decomposition \(
  \begin{bmatrix}
    \sqrt{\tilde{\symbf{W}}} \symbf{X} \symbf{C} \\
    \symbf{B}
  \end{bmatrix}
  = \symbf{Q} \symbf{R},
  \)
  where \(\symbf{B}\) is any matrix square root such that \(\symbf{B}^{\text{T}} \symbf{B} = \symbf{S}_\uplambda\).
\item Letting \(\symbf{Q}_1\) denote the first \(n\) rows of \(\symbf{Q}\), form the symmetric eigen-decomposition \\
  \(\symbf{Q}_1^{\text{T}} \symbf{I}^- \symbf{Q}_1 + \symbf{R}^{-\text{T}} \symbf{E} \symbf{R}^{-1} = \symbf{U} \symbf{\Lambda} \symbf{U}^{\text{T}}\).
\item Hence define \(\symbf{P} = \symbf{R}^{-1} \symbf{U}(\symbf{I} - \symbf{\Lambda})^{-1/2}\) and \(\symbf{K} = \symbf{Q}_1 \symbf{U} (\symbf{I} - \symbf{\Lambda})^{-1/2}\).
\item Update the estimate of \(\symbf{\beta}\) as \(\symbf{\beta}^{[k + 1]} = \symbf{\beta}^{[k]} + \symbf{P} \symbf{K}^{\text{T}} \sqrt{\tilde{\symbf{W}}} \tilde{\symbf{z}} - \symbf{P} \symbf{P}^{\text{T}} \symbf{S}_\uplambda \symbf{\beta}^{[k]}\) and increment \(k\).
\end{enumerate}
The SCAMs presented in this study optimize the GCV/un-biased risk estimator score \parencite{Craven1979,Wahba1990} for selecting the estimate of the smoothing parameter vector \(\mathbf{\uplambda}\).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "MasArThesis.tex"
%%% End:
