\subsection{Explanation of GAMs}

Generalized Additive Models (GAMs) are an extension of Generalized Linear Models (GLMs), which in turn build upon Linear Models (LMs).
% In order to explain GAMs, I will therefore first describe LMs and GLMs, before going into detail about GAMs.

% \subsubsection{Distributional assumptions in Linear Models}

A simple LM assumes a metric random variable \(Y\) to linearly depend upon the metric variable \(X\), such that
\begin{equation}
  \label{eq:LinearModel}
  Y = X \beta + \varepsilon
\end{equation}
where \(\beta\) is the coefficient of \(X\) to be estimated and \(\varepsilon_i\) is an independent random variable such that \(\mathbb{E}\bigl(\varepsilon_i\bigr) = 0\) and \(\mathbb{E}\bigl(\varepsilon_i^2\bigr) = \sigma^2\).  To allow testing of hypotheses related to the model described by \RefEq{eq:LinearModel}, additional assumptions about the distribution of \(Y\) and \(\varepsilon\) need to be made.  Specifically, 2 assumptions are made: the residual term \(\varepsilon\) is assumed to follow normal distribution with a mean of zero and a variance of \(\sigma^2\): \(\varepsilon \sim N\bigl(0, \sigma^2\bigr)\); and the response variable \(Y\) is assumed to follow normal distribution with a mean equal to the product of independent variable \(X\) and parameter \(\beta\) and a variance of \(\sigma^2\): \(Y \sim N\bigl(X \beta, \sigma^2\bigr) \) \parencite{Wood2006,Burkschat2012}.

% \subsubsection{Distributional assumptions in Generalized Linear Models}

A GLM has the structure
\begin{equation}
  \label{eq:GeneralizedLinearModel}
  g\bigl(\mu_i\bigr) = \mathbf{x}_i \symbf{\beta}~,  %% Command "\symbf" is provided by package "unicode-math" (see unicode-math manual p. 11)
\end{equation}
where \(\mu_i\) is the expectation of response variable \(Y_i\), i.e., \(\mu_i \equiv \mathbb{E}\bigl(Y_i\bigr)\), \(g\) is a smooth monotonic ``link function'', \(\mathbf{x}_i\) is the \(i^{\text{th}}\) row of model matrix \(\mathbf{X}\), and \(\symbf{\beta}\) is a vector of unknown parameters \parencite{Wood2006}.
GLMs are an extension of LMs insofar as they also also center around a ``linear predictor'', \(\mathbf{X}\symbf{\beta}\), but allow other link functions than the identity function and allow the distribution of the dependent variable to be any distribution from the exponential family, instead of only the normal distribution.  The exponential family consists of distributions whose probability density function can be written as
\begin{equation}
  \label{eq:ExponentialFamilyProbabilityDensityFunction}
  f_{\theta}\bigl(y\bigr) = \exp \bigg( \frac{y \theta - b(\theta)}{a(\Phi)} + c(y, \Phi)\bigg)~,
\end{equation}
where \(a\), \(b\), and \(c\) are arbitrary functions, \(\Phi\) is an arbitrary ``scale'' parameter, and \(\theta\) is the so-called ``canonical parameter'' of the distribution \parencite{Wood2006}.

Building upon GLMs, a GAM has a structure similar to
\begin{equation}
  \label{eq:GeneralizedAdditiveModel}
  g\bigl(\mu_i\bigr) = \symbf{X}_i^* \symbf{\theta} + f_{1}\bigl(x_{1i}\bigr) + f_{2}\bigl(x_{2i}\bigr) + f_{3}\bigl(x_{3i}, x_{4i}\bigr) + \ldots~,
\end{equation}
where \(\mu_i\) again is the expectation of response variable \(Y_i\), i.e., \(\mu_i \equiv \mathbb{E}\bigl(Y_i\bigr)\), the response variable \(Y_i\) follows any exponential family distribution, \(X_i^*\) is a row of the model matrix for any strictly parametric model components, \(\symbf{\theta}\) is the corresponding parameter vector, and the \(f_j\) are smooth functions of the covariates, \(x_k\) \parencite{Wood2006}.  %% Continue here (Wood (2006) p. 119)

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "MasArThesis.tex"
%%% End:
