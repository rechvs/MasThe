\subsection{Explanation of GAMs}

Generalized Additive Models (GAMs) \parencite{Wood2006,Hastie1991} are an extension of Generalized Linear Models (GLMs) \parencite{Nelder1972}, which in turn build upon Linear Models (LMs).

A simple LM assumes a metric random variable \(Y\) to linearly depend upon the metric variable \(X\), such that
\begin{equation}
  \label{eq:LinearModel}
  Y = X \beta + \varepsilon
\end{equation}
where \(\beta\) is the coefficient of \(X\) to be estimated and \(\varepsilon_i\) is an independent random variable such that \(\mathbb{E}\bigl(\varepsilon_i\bigr) = 0\) and \(\mathbb{E}\bigl(\varepsilon_i^2\bigr) = \sigma^2\).  To allow testing of hypotheses related to the model described by \RefEq{eq:LinearModel}, additional assumptions about the distribution of \(Y\) and \(\varepsilon\) need to be made.  Specifically, 2 assumptions are made: the residual term \(\varepsilon\) is assumed to follow normal distribution with a mean of zero and a variance of \(\sigma^2\): \(\varepsilon \sim N\bigl(0, \sigma^2\bigr)\); and the response variable \(Y\) is assumed to follow normal distribution with a mean equal to the product of independent variable \(X\) and parameter \(\beta\) and a variance of \(\sigma^2\): \(Y \sim N\bigl(X \beta, \sigma^2\bigr) \) \parencite{Wood2006,Burkschat2012}.

A GLM has the structure
\begin{equation}
  \label{eq:GeneralizedLinearModel}
  g\bigl(\mu_i\bigr) = \symbf{x}_i \symbf{\beta},
\end{equation}
where \(\mu_i\) is the expectation of response variable \(Y_i\), i.e., \(\mu_i \equiv \mathbb{E}\bigl(Y_i\bigr)\), \(g\) is a smooth monotonic ``link function'', \(\symbf{x}_i\) is the \(i^{\text{th}}\) row of model matrix \(\symbf{X}\), and \(\symbf{\beta}\) is a vector of unknown parameters \parencite{Wood2006,Nelder1972}.
GLMs are an extension of LMs insofar as they also also center around a ``linear predictor'', \(\symbf{X}\symbf{\beta}\), but allow other link functions than the identity function and allow the distribution of the dependent variable to be any distribution from the exponential family, instead of only the normal distribution.  The exponential family consists of distributions whose probability density function can be written as
\begin{equation}
  \label{eq:ExponentialFamilyProbabilityDensityFunction}
  f_{\theta}\bigl(y\bigr) = \exp \bigg( \frac{y \theta - b(\theta)}{a(\Phi)} + c(y, \Phi)\bigg),
\end{equation}
where \(a\), \(b\), and \(c\) are arbitrary functions, \(\Phi\) is an arbitrary ``scale'' parameter, and \(\theta\) is the so-called ``canonical parameter'' of the distribution \parencite{Wood2006}.

Building upon GLMs, a GAM has a structure similar to
\begin{equation}
  \label{eq:GAM}
  g\bigl(\mu_i\bigr) = \symbf{x}_i^* \symbf{\theta} + f_{1}\bigl(x_{1, i}\bigr) + f_{2}\bigl(x_{2, i}\bigr) + f_{3}\bigl(x_{3, i}, x_{4, i}\bigr) + \ldots,
\end{equation}
where \(\mu_i\) again is the expectation of response variable \(Y_i\), i.e., \(\mu_i \equiv \mathbb{E}\bigl(Y_i\bigr)\), the response variable \(Y_i\) follows any exponential family distribution, \(\symbf{x}_i^*\) is a row of the model matrix for any strictly parametric model components, \(\symbf{\theta}\) is the corresponding parameter vector, and the \(f_j\) are smooth functions of the covariates, \(x_k\) \parencite{Wood2006}.  A smooth function may be considered an estimate of the acutal functional relationship between the response variable and the predictor variable to which the smooth function is applied \parencite{Hastie1991}.  Unlike LMs or GLMs, a smooth function is nonparametric, i.e., it does not assume the response variable to follow any specific distribution.
As an explanatory example, consider a model containing one smooth function of one predictor variable with the identity function as the link function,
\begin{equation}
  \label{eq:GAMSimple}
  y_i = f\bigl(x_i\bigr) + \varepsilon_i,
\end{equation}
where \(y_i\) is a response variable, \(x_i\) is a predictor variable, \(f\) is a smooth function, and the \(\varepsilon_i\) follow the normal distribution with a mean of zero and a variance of \(\sigma^2\) \parencite{Wood2006}.  The \(x_i\) lie in the interval \([0, 1]\).  In order to be able to estimate \(f\), a space of known functions, of which \(f\) (or its estimate) is assumed to be an element, needs to be chosen, such that
\begin{equation}
  \label{eq:SmoothFunctionBasis}
  f(x) = \sum_{i = 1}^q b_i(x)\beta_i,
\end{equation}
where \(q\) is the number of observations, \(b_i(x)\) is the \(i^{\text{th}}\) element of the chosen function space, and \(\beta_i\) is an unknown parameter \parencite{Wood2006}.  Substituting \RefEq{eq:SmoothFunctionBasis} into \RefEq{eq:GAMSimple} yields a linear model which can then be fitted using methods for LMs or GLMs.

\subsubsection{Thin-plate regression splines}

The GAMs presented in this study employ thin plate regression splines \parencite{Wood2003} for creating the basis of all their smooth functions.  Thin plate regression splines are based on thin plate splines \parencite{Duchon1977}.  In order to explain thin plate splines, consider the problem of estimating the smooth function \(g(x)\), based on \(n\) observations \((y_i, \symbf{x}_i)\) such that
\begin{equation}
  \label{eq:ThinPlateSplinesModel}
  y_i = g(\symbf{x}_i) + \varepsilon_i,
\end{equation}
where \(\varepsilon_i\) is a random error term and where \(\symbf{x}\) is a \(d\)-vector \((d \leq n)\) \parencite{Wood2006}.  Function \(g\) is then estimated by finding the function \(\hat{f}\) which minimizes
\begin{equation}
  \label{eq:ThinPlateSplinesTermToMinimize}
  \norm{\symbf{y} - \symbf{f}}^2 + \uplambda J_{m, d}(f),
\end{equation}
where \(\symbf{y}\) is the vector of \(y_i\) data, \(\symbf{f} = \bigl(f\bigl(\symbf{x}_1\bigr), f\bigl(\symbf{x}_2\bigr), \ldots, f\bigl(\symbf{x}_n\bigr)\bigr)^{\text{T}}\), \(\uplambda\) is a smoothing parameter controlling the tradeoff between data fitting and smoothness of \(f\), and \(J_{m, d}(f)\) is a penalty functional measuring the ``roughness'' of \(f\) \parencite{Wood2006}.  The penalty is defined as
\begin{equation}
  \label{eq:ThinPlateSplinesPenalty}
  J_{m, d} = \int{\hspace{-3mm}}\cdots\int_{\mathfrak{R}^d} \sum_{\nu_1 + \ldots + \nu_d = m}\frac{m!}{\nu_1! \ldots \nu_d!} \left(\frac{\partial^mf}{\partial x_1^{\nu_1} \ldots \partial x_d^{\nu_d}}\right)^2 dx_1 \ldots dx_d~
\end{equation}
\parencite{Wood2006}.  If the restriction \(2m > d\) is imposed, the function minimizing \RefEq{eq:ThinPlateSplinesTermToMinimize} has the form
\begin{equation}
  \label{eq:ThinPlateSplinesMinimizingFunctionForm}
  \hat{f}(\symbf{x}) = \sum_{i = 1}^n \delta_i\eta_{m, d}\left(\norm{\symbf{x} - \symbf{x}_i}\right) + \sum_{j = 1}^M\alpha_j\Phi_j(\symbf{x}),
\end{equation}
where \(\symbf{\delta}\) and \(\symbf{\alpha}\) are vectors of coefficients to be estimated, \(\symbf{\delta}\) being subject to the linear constraints that \(\symbf{T}^{\text{T}}\symbf{\delta} = 0\), where \(T_{i, j} = \Phi_j\bigl(\symbf{x}_i\bigr)\) \parencite{Wood2006}.
The \(M = \dbinom{m + d - 1}{d}\) functions, \(\Phi_i\), are linearly independent polynomials spanning the space of polynomials \(\mathfrak{R}^d\) of degree less than \(m\).  %% I need to use "\dbinom...", because XITS Math (and all OpenType fonts for that matter) in combination with my version of LuaLaTeX cannot properly place the binomial coefficients when using "\tbinom...".
The \(\Phi_i\) span the space of functions for which \(J_{m, d}\) is zero.  The \(\eta_{m, d}\) functions are defined as
\begin{equation}
  \label{eq:ThinPlateSplinesEtaFunctionDefinition}
  \eta_{m, d}(r) =
  \begin{cases}
    \frac{(-1)^{m + 1 + d / 2}}{2^{2m - 1}\uppi^{d / 2}(m - 1)!(m - d / 2)!}r^{2m - d}\log(r) & \text{if \(d\) is even} \\
    \frac{\upGamma (d / 2 - m)}{2^{2m}\uppi^{d / 2}(m - 1)!}r^{2m - d} & \text{if \(d\) is odd}
  \end{cases}
\end{equation}
\parencite{Wood2006}.  If matrix \(\symbf{E}\) is defined by \(E_{i, j} \equiv \eta_{m, d}\Bigl(\norm{\symbf{x}_i - \symbf{x}_j}\Bigr)\), then the thin plate spline fitting problem becomes

\begin{equation}
  \label{eq:ThinPlateSplineSimplifiedFittingProblem}
  \text{minimize } \norm{\symbf{y} - \symbf{E}\symbf{\delta} - \symbf{T}\symbf{\alpha}}^2 + \uplambda \symbf{\delta}^{\text{T}}\symbf{E}\symbf{\delta} \text{ subject to } \symbf{T}^{\text{T}}\symbf{\delta} = 0,
\end{equation}
with respect to \(\symbf{\delta}\) and \(\symbf{\alpha}\) \parencite{Wood2006}.

Thin plate regression splines build upon thin plate splines by truncating the space of the components with parameter \(\symbf{\delta}\), without changing the \(\symbf{\alpha}\) components.  Let \(\symbf{E} = \symbf{U D U}^{\text{T}}\) be the eigen-decomposition of \(\symbf{E}\), so that \(\symbf{D}\) is a diagonal matrix of eigenvalues of \(\symbf{E}\) arranged so that \(\abs{D_{i, i}} \geq \abs{D_{i - 1, i - 1}}\) and the columns of \(\symbf{U}\) are the corresponding eigenvectors. Furthermore, let \(\symbf{U}_k\) denote the matrix consisting of the first \(k\) columns of \(\symbf{U}\) and let \(\symbf{D}_k\) denote the top right \(k \times k\) submatrix of \(\symbf{D}\).  By writing \(\symbf{\delta} = \symbf{U}_k \symbf{\delta}_k\), one can restrict \(\symbf{\delta}\) to the columns space of \(\symbf{U}_k\), by which \RefEq{eq:ThinPlateSplineSimplifiedFittingProblem} becomes
\begin{equation}
  \label{eq:ThinPlateRegressionSplinesSimplifiedFittingProblem}
  \text{minimize } \norm{\symbf{y} - \symbf{U}_k\symbf{D}_k\symbf{\delta}_k - \symbf{T \alpha}}^2 + \uplambda \symbf{\delta}_k^{\text{T}} \symbf{D}_k \symbf{\delta}_k \text{ subject to } \symbf{T}^{\text{T}} \symbf{U}_k \symbf{\delta}_k = 0
\end{equation}
with respect to \(\symbf{\delta}_k\) and \(\symbf{\alpha}\) \parencite{Wood2006}.  In order to absorb the constraints, an orthogonal column basis \(\symbf{Z}_k\) must be found, such that \(\symbf{T}^{\text{T}} \symbf{U}_k \symbf{Z}_k = 0\).  By writing \(\symbf{\delta}_k = \symbf{Z}_k \tilde{\symbf{\delta}}\) one can restrict \(\symbf{\delta}_k\) to this space.  This yields the unconstrained problem that must be solved to fit the rank \(k\) approximation to the smoothing spline:
\begin{equation}
  \label{eq:ThinPlateRegressionSplinesUnconstrainedFittingProblem}
  \text{minimize } \norm{\symbf{y} - \symbf{U}_k \symbf{D}_k \symbf{Z}_k \tilde{\symbf{\delta}} - \symbf{T} \symbf{\alpha}}^2 + \uplambda \tilde{\symbf{\delta}}^{\text{T}} \symbf{Z}_k^{\text{T}} \symbf{D}_k \symbf{Z}_k \tilde{\symbf{\delta}}
\end{equation}
with respect to \(\tilde{\symbf{\delta}}\) and \(\symbf{\alpha}\) \parencite{Wood2006}.

\subsubsection{Generalized cross validation}

For the GAMs presented in this study, estimation of the smoothing parameter \(\uplambda\) for each smooth function was achieved via generalized cross validation (GCV), which builds upon cross validation.  In cross validation, each pair of observations \(\bigl(x_i, y_i\bigr)\) is left out one at a time while estimating the smooth function at \(x_i\) using the remaining \(n - 1\) observations.  This way, cross validation mimics the use of training and test data for model fitting.  The quantity of interest is the cross validation sum of squares
\begin{equation}
  \label{eq:CrossValidationSumOfSquares01}
  CV(\uplambda) = \frac{1}{n} \sum_{i = 1}^n\Bigl(y_i - \hat{f}_\uplambda^{-i}\bigl(x_i\bigr)\Bigr)^2,
\end{equation}
where \(n\) is the total number of observations, \(y_i\) is the \(i^{\text{th}}\) observation of the response variable \(Y\), \(x_i\) is the \(i^{\text{th}}\) observation of the response variable \(X\), and \(\hat{f}_\uplambda^{-i}\bigl(x_i\bigr)\) is the smooth function fit at \(x_i\), obtained with smoothing parameter \(\uplambda\) and by leaving out \(\bigl(x_i, y_i\bigr)\) \parencite{Hastie1991}.  The cross validation sum of squares is computed for several values of \(\uplambda\) over a suitable range and the smoothing parameter value \(\hat{\uplambda}\) which minimizes \(CV(\uplambda)\) is selected.  A formal definition of \(\hat{f}_\uplambda^{-i}\bigl(x_i\bigr)\) for linear smoothers is
\begin{equation}
  \label{eq:CrossValidationFormalDefinitionOfJackknifedFit1}
  \hat{f}_\uplambda^{-i}\bigl(x_i\bigr) = \sum_{\substack{j = 1 \\ j \not = i}}^n \frac{S_{i, j}(\uplambda)}{1 - S_{i, i}(\uplambda)} y_i,
\end{equation}
where \(S_{i, j}(\uplambda)\) is the element of row \(i\) and column \(j\) of the smoother matrix \(\symbf{S}_\uplambda\) for smoothing parameter \(\uplambda\).  The smoother matrix \(\symbf{S}\) is the matrix which satisfies the equation
\begin{equation}
  \label{eq:SmootherMatrixDefinition}
  \hat{\symbf{f}} = \symbf{S} \symbf{y},
\end{equation}
where \(\hat{\symbf{f}}\) is the vector of smooth function fits at observations \(x_1, \ldots, x_n\) and \(\symbf{y}\) is the vector of observations of the response variable.
Equation \ref{eq:CrossValidationFormalDefinitionOfJackknifedFit1} implies that
\begin{equation}
  \label{eq:CrossValidationFormalDefinitionOfJackknifedFit2}
  \hat{f}_\uplambda^{-i}\bigl(x_i\bigr) = \sum_{\substack{j = 1 \\ j \not = i}}^n S_{i, j}(\uplambda) y_i +S_{i, i}(\uplambda) \hat{f}_\uplambda^{-i}\bigl(x_i\bigr),
\end{equation}
which in turn implies that
\begin{equation}
  \label{eq:CrossValidationFormalDefinitionOfJackknifedFit3}
  y_i - \hat{f}_\uplambda^{-i}\bigl(x_i\bigr) = \frac{y_i - \hat{f}_\uplambda\bigl(x_i\bigr)}{1 - S_{i, i}(\uplambda)}.
\end{equation}
Substituting \RefEq{eq:CrossValidationFormalDefinitionOfJackknifedFit3} into \RefEq{eq:CrossValidationSumOfSquares01} yields
\begin{equation}
  \label{eq:CrossValidationSumOfSquares02}
  CV(\uplambda) = \frac{1}{n} \sum_{i = 1}^n\Biggl(\frac{y_i - \hat{f}_\uplambda\bigl(x_i\bigr)}{1 - S_{i, i}(\uplambda)}\Biggr)^2.
\end{equation}
GCV replaces \(S_{i, i}(\uplambda)\) in \RefEq{eq:CrossValidationSumOfSquares02} with its mean value \(\tr\bigl(\symbf{S}_\uplambda\bigr) \big/ n\), since it is easier to compute:
\begin{equation}
  \label{eq:GeneralizedCrossValidation}
  GCV(\uplambda) = \frac{1}{n} \sum_{i = 1}^n\Biggl(\frac{y_i - \hat{f}_\uplambda\bigl(x_i\bigr)}{1 - \tr\bigl(\symbf{S}_\uplambda\bigr) \big/ n}\Biggr)^2,
\end{equation}
where \(\tr(\symbf{A})\) is the trace or sum of the diagonal elements of square matrix \(\symbf{A}\) \parencite{Hastie1991}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "MasArThesis.tex"
%%% End:
